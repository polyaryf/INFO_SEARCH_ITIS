import os  # –†–∞–±–æ—Ç–∞ —Å —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–æ–π
from collections import Counter  # –£–¥–æ–±–Ω–æ —Å—á–∏—Ç–∞—Ç—å —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤
import numpy as np  # –î–ª—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
from sklearn.metrics.pairwise import cosine_similarity  # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
from nltk.tokenize import word_tokenize  # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞
from nltk.stem import WordNetLemmatizer  # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞
import nltk

# –°–∫–∞—á–∏–≤–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ä–µ—Å—É—Ä—Å—ã NLTK
nltk.download('punkt')
nltk.download('wordnet')


# ‚úÖ –§—É–Ω–∫—Ü–∏—è: –∑–∞–≥—Ä—É–∑–∫–∞ TF-IDF –≤–µ—Å–æ–≤ –∏–∑ —Ñ–∞–π–ª–æ–≤
def load_tf_idf_vectors(folder_path):
    vectors = {}        # –°–ª–æ–≤–∞—Ä—å: –∏–º—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ -> {—Ç–µ—Ä–º–∏–Ω: –≤–µ—Å}
    all_terms = set()   # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –≤—Å–µ—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –≤–æ –≤—Å—ë–º –∫–æ—Ä–ø—É—Å–µ

    for filename in os.listdir(folder_path):  # –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ
        if filename.endswith("_tf_idf_lemmas.txt"):
            doc_id = filename.replace("_tf_idf_lemmas.txt", "")  # –ü–æ–ª—É—á–∞–µ–º –∏–º—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –±–µ–∑ —Å—É—Ñ—Ñ–∏–∫—Å–∞
            vector = {}  # –í—Ä–µ–º–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞

            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as f:
                next(f)  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–æ–∫–∏: Term\tTF\tIDF
                for line in f:
                    parts = line.strip().split('\t')  # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Ç–∞–±—É–ª—è—Ü–∏–∏
                    if len(parts) == 3:
                        term, tf, idf = parts
                        weight = float(tf) * float(idf)  # TF-IDF –≤–µ—Å —Ç–µ—Ä–º–∏–Ω–∞
                        vector[term] = weight
                        all_terms.add(term)  # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫ —Ç–µ—Ä–º–∏–Ω–æ–≤

            vectors[doc_id] = vector  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞

    return vectors, sorted(all_terms)  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º: {–¥–æ–∫—É–º–µ–Ω—Ç: –≤–µ–∫—Ç–æ—Ä}, —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤


# ‚úÖ –§—É–Ω–∫—Ü–∏—è: –≤—ã–ø–æ–ª–Ω–∏—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É
def vector_search_from_files(query, vectors, all_terms):
    # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º –∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
    lemmatizer = WordNetLemmatizer()
    tokens = word_tokenize(query.lower())
    lemmas = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]

    # –°—á–∏—Ç–∞–µ–º TF –¥–ª—è –ª–µ–º–º –∑–∞–ø—Ä–æ—Å–∞
    query_counts = Counter(lemmas)
    total = len(lemmas)
    query_vector = {lemma: (query_counts[lemma] / total) for lemma in query_counts}

    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: —Å–ª–æ–≤–∞—Ä—å -> –≤–µ–∫—Ç–æ—Ä –ø–æ all_terms
    def to_vector(vec_dict):
        return np.array([vec_dict.get(term, 0) for term in all_terms])

    query_vec = to_vector(query_vector).reshape(1, -1)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ 2D-–≤–µ–∫—Ç–æ—Ä –¥–ª—è cosine_similarity

    document_scores = []  # –ú–∞—Å—Å–∏–≤ –ø–∞—Ä (–∏–º—è –¥–æ–∫—É–º–µ–Ω—Ç–∞, —Å—Ö–æ–¥—Å—Ç–≤–æ)

    for doc_id, doc_vector in vectors.items():
        doc_vec = to_vector(doc_vector).reshape(1, -1)
        sim = cosine_similarity(query_vec, doc_vec)[0][0]  # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        document_scores.append((doc_id, sim))

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
    ranked = sorted(document_scores, key=lambda x: x[1], reverse=True)

    print("\nüîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É:", query)
    for doc_id, score in ranked:
        if score > 0:
            print(f"{doc_id}: relevance = {score:.4f}")


# ‚úÖ –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
def main():
    folder_path = os.path.join("..", "Task4", "idf_lemmas")  # –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å tf-idf —Ñ–∞–π–ª–∞–º–∏

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –æ–±—â–∏–π —Å–ª–æ–≤–∞—Ä—å —Ç–µ—Ä–º–∏–Ω–æ–≤
    vectors, all_terms = load_tf_idf_vectors(folder_path)
     # –¶–∏–∫–ª: –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å—Ç—Ä–æ–∫—É –∏ –∏—â–µ–º
    while True:
        query = input("\n–í–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–∏–ª–∏ '–≤—ã—Ö–æ–¥'): ")
        if query.lower() in ["–≤—ã—Ö–æ–¥", "exit"]:
            print("‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
            break
        vector_search_from_files(query, vectors, all_terms)


# –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –≤ –ø—Ä–æ–≥—Ä–∞–º–º—É
if __name__ == "__main__":
    main()
